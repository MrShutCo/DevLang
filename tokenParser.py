from tokens import *
from grammar import *


# Stores a list of tokens and the position in the list
class TokenList:
    def __init__(self, tokens):
        # The list of tokens from the lexer
        self.tokens = tokens
        self.pos = 0

    def  __repr__(self):
        return str(self.tokens[self.pos:])

    def __str__(self):
        return str(self.tokens[self.pos:])

    # Return the next token
    def peek(self):
        if self.pos < len(self.tokens):
            return self.tokens[self.pos]
        raise Exception("Attempted to look past last token")

    # Move forward and return current token
    def next(self):
        self.pos += 1
        if self.pos-1 < len(self.tokens):
            return self.tokens[self.pos-1]
        raise Exception("Attempted to move past last token")

    # return next token and move forward if it matches the given type
    # if it does not match, return None
    def eat(self, token_type):
        if self.peek().type == token_type:
            return self.next()
        return None

    # the same as eat but it takes a list of tokens instead of a single one
    def eat_any(self, token_types):
        if self.peek().type in token_types:
            return self.next()
        return None

    # Ruturns a list of the next n tokens
    def peek_many(self, n):
        if self.pos+n < len(self.tokens):
            return self.tokens[self.pos:self.pos+n]
        raise Exception("Attempted to look past last token")


class Parser:
    def __init__(self, tokens):
        self.tokens = TokenList(tokens)

        print(self.tokens)
        print(self.tokens.peek())
        print(self.tokens.next())
        print(self.tokens.peek())
        print(self.tokens.peek_many(2))

# class Parser:
#     def __init__(self, tokens):
#         # The list of tokens generated by the lexer
#         self.tokens = tokens

#     def raise_error(self, token):
#         raise Exception(
#             "Parser error at {}.\nType: {}\nValue: {}"
#             .format(token.loc, token.type, token.value))

#     # See if the next token has the correct type
#     def eat(self, type_token):
#         token = self.peek()
#         if token.type == type_token:
#             return self.next()
#         else:
#             print(token)
#             self.raise_error(token)

#     # Token must match at least one of the types
#     def eat_any(self, type_tokens):
#         token = self.peek()
#         if token.type in type_tokens:
#             return self.next()
#         else:
#             self.raise_error(token)

#     # Pops a token from the stack (from the end of the list)
#     def next(self):
#         return self.tokens.pop()

#     # Inserts token back into the stack (at the end of the list)
#     def push(self, token):
#         self.tokens.append(token)

#     # Gets the next token without removing it from the list
#     def peek(self):
#         return self.tokens[-1]

#     # Gets the last n tokens
#     def peek_many(self, n):
#         return list(reversed(self.tokens[-n:]))

#     # TODO: Operator precedence
#     #       Parser combinators
#     def parse_AExpr(self):

#         left = None
#         tk = self.peek()
#         if tk.type == Special.OpenBracket: # If we encounter an open bracket
#             self.next()
#             expr = self.parse_AExpr()       # The brackets enclose an expression
#             self.eat(Special.CloseBracket) # It should end with a closing bracket
#             left = ABrackets(expr)
#         elif tk.type == Type.Integer: 
#             left = self.next()
#             tk = self.peek()
#             if self.is_operator(tk.type):  # Integer can be part of another expression
#                 self.next()
#                 op = tk.type
#                 right = self.parse_AExpr() # For now we don't care what is on the rhs
#                 left = ABinaryOp(op, AConstant(left.value), right)
#             else:
#                  left = AConstant(left.value)
        
#         tk = self.peek()
#         if self.is_operator(tk.type):  # left can be part of some outer expression
#             self.next()
#             op = tk.type
#             right = self.parse_AExpr()
#             return ABinaryOp(op, left, right)
#         else:
#             return left

#     def is_operator(self, type_token):
#         return type_token in [Operator.Add, Operator.Subtract,
#                               Operator.Divide, Operator.Multiply]

#     def parse(self):
#         # print(self.tokens, "\n")

#         print(self.parse_AExpr())
